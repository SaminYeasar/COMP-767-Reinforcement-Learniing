{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################\n",
    "##  Assignment 1\n",
    "##  Problem 2(a)\n",
    "##  Samin Yeasar Arnob\n",
    "##  McGill ID: 260800927\n",
    "##  COMP 767- Reinforcement Learning Winter 2019\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid World Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Original Grid world environment was taken from link: https://github.com/dennybritz/reinforcement-learning \\\n",
    "\n",
    "I modified it with: \n",
    "\n",
    "(1) \"Terminal states\" at being \n",
    "\n",
    "    -Upper left (T1) with reward 1.0\n",
    "    - Upper right (T2) with reward 10.0\n",
    "    - Everywhere else reward 0.0\n",
    "(2) Inserted stochastic action condition:\n",
    "    - where it takes input \"prob_p = P \" probability of taking policy action\n",
    "    - and with probability (1-P) takes random action\n",
    "\n",
    "Environment Directory : gridworld.py \n",
    "\n",
    "\t\t\t\"\"\"\n",
    "            \n",
    "\t\t\t\tT1  o  o  T2\n",
    "                 o  o  o  o\n",
    "\t\t\t\t o  o  o  o\n",
    "\t\t\t\t x  o  o  o\n",
    "\t\t\t\n",
    "\t\t\t\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "I have written my findings from the experments after each section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For grid size 5x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "#\n",
    "# Modified Policy Iteration\n",
    "#\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\r\n",
      "Epoch 0\r\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\r\n",
      "Value Function:\r\n",
      "[[6.860e+00 3.180e+00 5.100e+00 1.969e+01 6.862e+01]\r\n",
      " [2.160e+00 1.340e+00 3.190e+00 9.240e+00 2.085e+01]\r\n",
      " [5.900e-01 4.400e-01 8.000e-01 3.340e+00 5.790e+00]\r\n",
      " [2.200e-01 2.300e-01 4.200e-01 1.100e+00 1.630e+00]\r\n",
      " [9.000e-02 9.000e-02 6.000e-02 1.400e-01 5.500e-01]]\r\n",
      "\r\n",
      "Policy has not converged, updating new policy\r\n",
      "---------------------------------------------\r\n",
      "Epoch 1\r\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\r\n",
      "Value Function:\r\n",
      "[[ 6.86  5.56 40.19 52.76 68.62]\r\n",
      " [ 5.56 24.52 32.55 42.73 55.58]\r\n",
      " [ 4.5  19.86 26.37 34.61 45.02]\r\n",
      " [ 3.65 16.09 21.36 28.04 36.47]\r\n",
      " [ 2.95 13.03 17.3  22.71 29.54]]\r\n",
      "\r\n",
      "Policy has not converged, updating new policy\r\n",
      "---------------------------------------------\r\n",
      "Epoch 2\r\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\r\n",
      "Value Function:\r\n",
      "[[ 6.86 30.27 40.19 52.76 68.62]\r\n",
      " [18.19 24.52 32.55 42.73 55.58]\r\n",
      " [14.73 19.86 26.37 34.61 45.02]\r\n",
      " [11.93 16.09 21.36 28.04 36.47]\r\n",
      " [ 9.67 13.03 17.3  22.71 29.54]]\r\n",
      "\r\n",
      "Policy Iteration Converged after 3 iteration\r\n",
      "Reshaped Grid- Final Policy (0=up, 1=right, 2=down, 3=left):\r\n",
      "[[0 1 1 1 0]\r\n",
      " [1 1 1 1 0]\r\n",
      " [0 0 1 1 0]\r\n",
      " [1 1 1 1 0]\r\n",
      " [1 1 1 1 0]]\r\n",
      "\r\n",
      "Reshaped Grid- Final Value Function:\r\n",
      "[[ 6.86 30.27 40.19 52.76 68.62]\r\n",
      " [18.19 24.52 32.55 42.73 55.58]\r\n",
      " [14.73 19.86 26.37 34.61 45.02]\r\n",
      " [11.93 16.09 21.36 28.04 36.47]\r\n",
      " [ 9.67 13.03 17.3  22.71 29.54]]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "grid_size = Input Modifies grid shape \"default = 5\"\n",
    "prob_p = probability of choosing stochastic action \"default = 0.9\"\n",
    "stop_itr = finite number of backups to the value function before an improvement step \"defalt=10\" \n",
    "Final_Result = if True; gives final Policy and Value function \"default = False\"\n",
    "\"\"\"\n",
    "!python Modified_Policy_Iteration.py --grid_size=5 --prob_p=0.9 --stop_itr=10 --Final_Result=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\r\n",
      "Epoch 0\r\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\r\n",
      "Value Function:\r\n",
      "[[6.860e+00 2.000e-02 2.170e+00 1.203e+01 6.862e+01]\r\n",
      " [1.000e-02 7.000e-02 4.600e-01 2.230e+00 1.361e+01]\r\n",
      " [1.000e-02 3.000e-02 1.300e-01 8.200e-01 2.840e+00]\r\n",
      " [0.000e+00 1.000e-02 4.000e-02 1.500e-01 5.500e-01]\r\n",
      " [0.000e+00 0.000e+00 0.000e+00 2.000e-02 1.300e-01]]\r\n",
      "\r\n",
      "Policy has not converged, updating new policy\r\n",
      "---------------------------------------------\r\n",
      "Epoch 1\r\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\r\n",
      "Value Function:\r\n",
      "[[ 6.86  5.18 24.31 41.03 68.62]\r\n",
      " [ 5.18  8.97 15.32 25.85 43.23]\r\n",
      " [ 3.26  5.65  9.65 16.29 27.23]\r\n",
      " [ 2.05  3.56  6.08 10.26 17.16]\r\n",
      " [ 1.29  2.24  0.55  6.46 10.81]]\r\n",
      "\r\n",
      "Policy has not converged, updating new policy\r\n",
      "---------------------------------------------\r\n",
      "Epoch 2\r\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\r\n",
      "Value Function:\r\n",
      "[[ 6.86  5.18 24.31 41.03 68.62]\r\n",
      " [ 5.18  8.97 15.32 25.85 43.23]\r\n",
      " [ 3.26  5.65  9.65 16.29 27.23]\r\n",
      " [ 2.05  3.56  6.08 10.26 17.16]\r\n",
      " [ 1.29  2.24  1.41  6.46 10.81]]\r\n",
      "\r\n",
      "Policy Iteration Converged after 3 iteration\r\n",
      "Reshaped Grid- Final Policy (0=up, 1=right, 2=down, 3=left):\r\n",
      "[[0 2 1 1 0]\r\n",
      " [1 1 0 1 0]\r\n",
      " [1 1 1 1 0]\r\n",
      " [1 1 1 0 0]\r\n",
      " [1 0 3 1 0]]\r\n",
      "\r\n",
      "Reshaped Grid- Final Value Function:\r\n",
      "[[ 6.86  5.18 24.31 41.03 68.62]\r\n",
      " [ 5.18  8.97 15.32 25.85 43.23]\r\n",
      " [ 3.26  5.65  9.65 16.29 27.23]\r\n",
      " [ 2.05  3.56  6.08 10.26 17.16]\r\n",
      " [ 1.29  2.24  1.41  6.46 10.81]]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python Modified_Policy_Iteration.py --grid_size=5 --prob_p=0.7 --stop_itr=10 --Final_Result=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################\n",
    "\n",
    "# Report:\n",
    "\n",
    "##########################\n",
    "\n",
    "### Even though the Policy isn't optimal but it finds it's way to terminal state as value function around Terminal states are higher. In this case when we choose stochastic action probability 0.7, gives higher value function around the upper right terminal comparing other states. For p = 0.7 and 0.9 policy tries to end up at upper right terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#\n",
    "# Policy Iteration\n",
    "#\n",
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[1.000e+01 6.310e+00 1.134e+01 3.216e+01 9.996e+01]\n",
      " [3.430e+00 3.510e+00 6.210e+00 1.538e+01 3.190e+01]\n",
      " [1.000e-02 1.390e+00 2.690e+00 5.660e+00 1.030e+01]\n",
      " [2.200e-01 6.300e-01 1.150e+00 2.200e+00 3.000e+00]\n",
      " [2.000e-01 3.800e-01 6.600e-01 1.080e+00 1.390e+00]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 1\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.   53.11 65.58 80.96 99.96]\n",
      " [ 8.1  43.02 53.12 65.58 80.97]\n",
      " [ 1.05 34.85 43.02 53.12 65.58]\n",
      " [22.86 28.23 34.85 43.03 53.12]\n",
      " [18.52 22.86 28.23 34.85 43.03]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 2\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.   53.11 65.58 80.96 99.96]\n",
      " [34.85 43.02 53.12 65.58 80.97]\n",
      " [ 1.05 34.85 43.02 53.12 65.58]\n",
      " [22.86 28.23 34.85 43.03 53.12]\n",
      " [18.52 22.86 28.23 34.85 43.03]]\n",
      "\n",
      "Policy Iteration Converged after 3 iteration\n",
      "Reshaped Grid- Final Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 1 1 1 0]\n",
      " [1 1 1 1 0]\n",
      " [0 1 1 1 0]\n",
      " [1 1 1 1 0]\n",
      " [1 1 1 1 0]]\n",
      "\n",
      "Reshaped Grid- Final Value Function:\n",
      "[[10.   53.11 65.58 80.96 99.96]\n",
      " [34.85 43.02 53.12 65.58 80.97]\n",
      " [ 1.05 34.85 43.02 53.12 65.58]\n",
      " [22.86 28.23 34.85 43.03 53.12]\n",
      " [18.52 22.86 28.23 34.85 43.03]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python Policy_Iteration.py --grid_size=5 --prob_p=0.9 --Final_Result=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[1.000e+01 1.640e+00 3.500e-01 1.500e-01 9.996e+01]\n",
      " [1.000e-02 4.000e-02 7.000e-02 4.200e-01 2.400e+00]\n",
      " [0.000e+00 1.000e-02 4.000e-02 1.500e-01 4.800e-01]\n",
      " [0.000e+00 0.000e+00 0.000e+00 2.000e-02 9.000e-02]\n",
      " [0.000e+00 0.000e+00 0.000e+00 0.000e+00 2.000e-02]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 1\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[1.000e+01 6.300e+00 3.970e+00 3.570e+00 9.996e+01]\n",
      " [3.600e-01 5.700e-01 2.500e+00 5.670e+00 9.000e+00]\n",
      " [2.200e-01 3.600e-01 2.250e+00 3.570e+00 5.670e+00]\n",
      " [1.400e-01 3.500e-01 1.420e+00 2.250e+00 3.570e+00]\n",
      " [3.500e-01 5.600e-01 8.900e-01 8.000e-02 2.250e+00]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 2\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[1.000e+01 6.300e+00 3.970e+00 3.570e+00 9.996e+01]\n",
      " [3.600e-01 5.700e-01 2.500e+00 5.670e+00 9.000e+00]\n",
      " [2.200e-01 3.600e-01 2.250e+00 3.570e+00 5.670e+00]\n",
      " [1.400e-01 3.500e-01 1.420e+00 2.250e+00 3.570e+00]\n",
      " [3.500e-01 5.600e-01 8.900e-01 8.000e-02 2.250e+00]]\n",
      "\n",
      "Policy Iteration Converged after 3 iteration\n",
      "Reshaped Grid- Final Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 2 0]\n",
      " [1 1 0 1 1]\n",
      " [1 0 1 1 0]\n",
      " [0 2 1 1 0]\n",
      " [1 1 0 0 0]]\n",
      "\n",
      "Reshaped Grid- Final Value Function:\n",
      "[[1.000e+01 6.300e+00 3.970e+00 3.570e+00 9.996e+01]\n",
      " [3.600e-01 5.700e-01 2.500e+00 5.670e+00 9.000e+00]\n",
      " [2.200e-01 3.600e-01 2.250e+00 3.570e+00 5.670e+00]\n",
      " [1.400e-01 3.500e-01 1.420e+00 2.250e+00 3.570e+00]\n",
      " [3.500e-01 5.600e-01 8.900e-01 8.000e-02 2.250e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python Policy_Iteration.py --grid_size=5 --prob_p=0.7 --Final_Result=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#################\n",
    "\n",
    "# Report:\n",
    "\n",
    "#################\n",
    "\n",
    "### Comparing with other algorithms Policy Iteration gives the optimal policy when probability of taking stochastic action is 0.9. For p = 0.7 for some reason policy indicates to terminate at upper left terminal even though upper right terminal state has higher value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#\n",
    "# Value Iteration\n",
    "#\n",
    "##########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function after 1 iteration\n",
      "[[ 1.    0.81  0.    0.   10.  ]\n",
      " [ 0.81  0.66  0.53  0.43  8.1 ]\n",
      " [ 0.66  0.53  0.43  0.35  6.56]\n",
      " [ 0.    0.43  0.35  0.28  5.31]\n",
      " [ 0.    0.35  0.28  0.23  4.3 ]]\n",
      "\n",
      "Value Function after 21 iteration\n",
      "[[ 8.91 45.17 56.75 71.15 89.06]\n",
      " [29.05 36.59 45.97 57.63 72.14]\n",
      " [23.53 29.63 37.23 46.68 58.43]\n",
      " [19.06 24.   30.16 37.81 47.33]\n",
      " [15.44 19.44 24.43 30.63 38.34]]\n",
      "\n",
      "Value Function after 41 iteration\n",
      "[[ 9.87 52.17 64.53 79.8  98.67]\n",
      " [34.16 42.26 52.27 64.64 79.92]\n",
      " [27.67 34.23 42.34 52.36 64.74]\n",
      " [22.41 27.73 34.3  42.41 52.44]\n",
      " [18.15 22.46 27.78 34.35 42.47]]\n",
      "\n",
      "Value Function after 61 iteration\n",
      "[[ 9.98 53.03 65.48 80.85 99.84]\n",
      " [34.78 42.95 53.04 65.49 80.87]\n",
      " [28.17 34.79 42.96 53.05 65.5 ]\n",
      " [22.82 28.18 34.8  42.97 53.06]\n",
      " [18.48 22.83 28.19 34.81 42.98]]\n",
      "\n",
      "Value Function after 81 iteration\n",
      "[[10.   53.13 65.59 80.98 99.98]\n",
      " [34.86 43.04 53.13 65.6  80.98]\n",
      " [28.23 34.86 43.04 53.13 65.6 ]\n",
      " [22.87 28.24 34.86 43.04 53.13]\n",
      " [18.52 22.87 28.24 34.86 43.04]]\n",
      "\n",
      "Value Function after 101 iteration\n",
      "[[ 10.    53.14  65.61  81.   100.  ]\n",
      " [ 34.87  43.05  53.14  65.61  81.  ]\n",
      " [ 28.24  34.87  43.05  53.14  65.61]\n",
      " [ 22.88  28.24  34.87  43.05  53.14]\n",
      " [ 18.53  22.88  28.24  34.87  43.05]]\n",
      "\n",
      "Converged after 110 number of iterations\n",
      "Reshaped Grid- Final Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 1 1 1 0]\n",
      " [1 1 0 1 0]\n",
      " [1 1 0 1 0]\n",
      " [1 1 1 1 0]\n",
      " [1 0 1 1 0]]\n",
      "\n",
      "Reshaped Grid- Final Value Function:\n",
      "[[ 10.    53.14  65.61  81.   100.  ]\n",
      " [ 34.87  43.05  53.14  65.61  81.  ]\n",
      " [ 28.24  34.87  43.05  53.14  65.61]\n",
      " [ 22.88  28.24  34.87  43.05  53.14]\n",
      " [ 18.53  22.88  28.24  34.87  43.05]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python Value_Iteration.py --grid_size=5 --prob_p=0.9 --Final_Result=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function after 1 iteration\n",
      "[[ 1.    0.    0.    0.   10.  ]\n",
      " [ 0.63  0.06  0.    0.    6.3 ]\n",
      " [ 0.    0.04  0.02  0.01  3.97]\n",
      " [ 0.    0.02  0.01  0.01  2.5 ]\n",
      " [ 0.    0.01  0.01  0.01  1.58]]\n",
      "\n",
      "Value Function after 21 iteration\n",
      "[[ 8.91  8.27 13.39 21.63 89.06]\n",
      " [ 5.61  5.21 21.63 34.86 56.11]\n",
      " [ 5.21  8.43 13.62 21.96 35.35]\n",
      " [ 3.28  5.31  8.58 13.84 22.27]\n",
      " [ 2.07  3.35  5.41  8.72 14.03]]\n",
      "\n",
      "Value Function after 41 iteration\n",
      "[[ 9.87  9.72 15.47 24.59 98.67]\n",
      " [ 6.22  6.13 24.59 39.1  62.16]\n",
      " [ 6.13  9.74 15.49 24.64 39.16]\n",
      " [ 3.86  6.14  9.76 15.52 24.67]\n",
      " [ 2.43  3.87  6.15  9.78 15.54]]\n",
      "\n",
      "Value Function after 61 iteration\n",
      "[[ 9.98  9.9  15.72 24.95 99.84]\n",
      " [ 6.29  6.24 24.95 39.62 62.9 ]\n",
      " [ 6.24  9.9  15.72 24.96 39.63]\n",
      " [ 3.93  6.24  9.9  15.72 24.96]\n",
      " [ 2.48  3.93  6.24  9.91 15.73]]\n",
      "\n",
      "Value Function after 81 iteration\n",
      "[[10.    9.92 15.75 25.   99.98]\n",
      " [ 6.3   6.25 25.   39.68 62.99]\n",
      " [ 6.25  9.92 15.75 25.   39.68]\n",
      " [ 3.94  6.25  9.92 15.75 25.  ]\n",
      " [ 2.48  3.94  6.25  9.92 15.75]]\n",
      "\n",
      "Value Function after 101 iteration\n",
      "[[ 10.     9.92  15.75  25.   100.  ]\n",
      " [  6.3    6.25  25.    39.69  63.  ]\n",
      " [  6.25   9.92  15.75  25.    39.69]\n",
      " [  3.94   6.25   9.92  15.75  25.  ]\n",
      " [  2.48   3.94   6.25   9.92  15.75]]\n",
      "\n",
      "Converged after 110 number of iterations\n",
      "Reshaped Grid- Final Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 1 1 2 0]\n",
      " [0 2 1 1 0]\n",
      " [1 1 1 1 0]\n",
      " [1 1 1 1 0]\n",
      " [0 1 1 1 0]]\n",
      "\n",
      "Reshaped Grid- Final Value Function:\n",
      "[[ 10.     9.92  15.75  25.   100.  ]\n",
      " [  6.3    6.25  25.    39.69  63.  ]\n",
      " [  6.25   9.92  15.75  25.    39.69]\n",
      " [  3.94   6.25   9.92  15.75  25.  ]\n",
      " [  2.48   3.94   6.25   9.92  15.75]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python Value_Iteration.py --grid_size=5 --prob_p=0.7 --Final_Result=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################\n",
    "\n",
    "# Report\n",
    "\n",
    "################\n",
    "\n",
    "### when P=0.7 both the terminal seems equally likely form value function where for P=0.9 upper right terminal state get's higher value function and makes the policy more likely to terminate with higher reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid size 50X50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "\n",
    "# Modified Policy Iteration\n",
    "\n",
    "###########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Epoch 0\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[6.860e+00 1.470e+00 2.000e-02 ... 4.550e+00 1.941e+01 6.862e+01]\n",
      " [1.840e+00 3.600e-01 9.000e-02 ... 2.890e+00 8.630e+00 1.947e+01]\n",
      " [5.000e-01 2.000e-01 6.000e-02 ... 1.120e+00 2.080e+00 5.500e-01]\n",
      " ...\n",
      " [0.000e+00 0.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 1\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[ 6.86  5.56  2.8  ... 40.19 52.76 68.62]\n",
      " [ 5.56  4.5   3.65 ... 32.55 42.73 55.58]\n",
      " [ 4.5   3.65  2.95 ... 26.37 34.61 28.04]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 2\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[ 6.86  5.56  2.8  ... 40.19 52.76 68.62]\n",
      " [ 5.56  4.5   3.65 ... 32.55 42.73 55.58]\n",
      " [ 4.5   3.65  2.95 ... 26.37 34.61 28.04]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 3\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[ 6.86  5.56  2.8  ... 40.19 52.76 68.62]\n",
      " [ 5.56  4.5   3.65 ... 32.55 42.73 55.58]\n",
      " [ 4.5   3.65  2.95 ... 26.37 34.61 28.04]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 4\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[ 6.86  5.56  2.8  ... 40.19 52.76 68.62]\n",
      " [ 5.56  4.5   3.65 ... 32.55 42.73 55.58]\n",
      " [ 4.5   3.65  2.95 ... 26.37 34.61 28.04]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 5\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[ 6.86  5.56  2.8  ... 40.19 52.76 68.62]\n",
      " [ 5.56  4.5   3.65 ... 32.55 42.73 55.58]\n",
      " [ 4.5   3.65  2.95 ... 26.37 34.61 28.04]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 6\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[ 6.86  5.56  2.8  ... 40.19 52.76 68.62]\n",
      " [ 5.56  4.5   3.65 ... 32.55 42.73 55.58]\n",
      " [ 4.5   3.65  2.95 ... 26.37 34.61 28.04]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 7\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[ 6.86  5.56  2.8  ... 40.19 52.76 68.62]\n",
      " [ 5.56  4.5   3.65 ... 32.55 42.73 55.58]\n",
      " [ 4.5   3.65  2.95 ... 26.37 34.61 28.04]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 8\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[ 6.86  5.56  2.8  ... 40.19 52.76 68.62]\n",
      " [ 5.56  4.5   3.65 ... 32.55 42.73 55.58]\n",
      " [ 4.5   3.65  2.95 ... 26.37 34.61 28.04]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 9\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[ 6.86  5.56  2.8  ... 40.19 52.76 68.62]\n",
      " [ 5.56  4.5   3.65 ... 32.55 42.73 55.58]\n",
      " [ 4.5   3.65  2.95 ... 26.37 34.61 28.04]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Reshaped Grid- Final Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 2 ... 1 1 0]\n",
      " [0 0 3 ... 1 1 0]\n",
      " [0 0 0 ... 1 0 3]\n",
      " ...\n",
      " [1 0 0 ... 1 1 0]\n",
      " [0 0 2 ... 1 0 0]\n",
      " [1 0 3 ... 1 1 0]]\n",
      "\n",
      "Reshaped Grid- Final Value Function:\n",
      "[[ 6.86  5.56  2.8  ... 40.19 52.76 68.62]\n",
      " [ 5.56  4.5   3.65 ... 32.55 42.73 55.58]\n",
      " [ 4.5   3.65  2.95 ... 26.37 34.61 28.04]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python Modified_Policy_Iteration.py --grid_size=50 --prob_p=0.9 --stop_itr=10 --Final_Result=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Epoch 0\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[6.860e+00 1.300e+00 2.500e-01 ... 2.610e+00 1.336e+01 6.862e+01]\n",
      " [1.150e+00 2.300e-01 8.000e-02 ... 1.300e+00 4.900e+00 1.542e+01]\n",
      " [1.900e-01 7.000e-02 1.000e-02 ... 4.300e-01 1.310e+00 2.850e+00]\n",
      " ...\n",
      " [0.000e+00 0.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 1\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[ 6.86  4.32  2.72 ... 24.31 41.03 68.62]\n",
      " [ 4.32  2.72  1.72 ... 15.32 25.85 43.23]\n",
      " [ 2.72  1.72  1.08 ...  9.65 16.29 27.23]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 2\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[ 6.86  4.32  2.72 ... 24.31 41.03 68.62]\n",
      " [ 4.32  2.72  1.72 ... 15.32 25.85 43.23]\n",
      " [ 2.72  1.72  1.08 ...  9.65 16.29 27.23]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 3\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[ 6.86  4.32  2.72 ... 24.31 41.03 68.62]\n",
      " [ 4.32  2.72  1.72 ... 15.32 25.85 43.23]\n",
      " [ 2.72  1.72  1.08 ...  9.65 16.29 27.23]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 4\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[ 6.86  4.32  2.72 ... 24.31 41.03 68.62]\n",
      " [ 4.32  2.72  1.72 ... 15.32 25.85 43.23]\n",
      " [ 2.72  1.72  1.08 ...  9.65 16.29 27.23]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 5\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[ 6.86  4.32  2.72 ... 24.31 41.03 68.62]\n",
      " [ 4.32  2.72  1.72 ... 15.32 25.85 43.23]\n",
      " [ 2.72  1.72  1.08 ...  9.65 16.29 27.23]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 6\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[ 6.86  4.32  2.72 ... 24.31 41.03 68.62]\n",
      " [ 4.32  2.72  1.72 ... 15.32 25.85 43.23]\n",
      " [ 2.72  1.72  1.08 ...  9.65 16.29 27.23]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 7\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[ 6.86  4.32  2.72 ... 24.31 41.03 68.62]\n",
      " [ 4.32  2.72  1.72 ... 15.32 25.85 43.23]\n",
      " [ 2.72  1.72  1.08 ...  9.65 16.29 27.23]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 8\n",
      "Running Modified Policy Iteration; Policy Evaluation is completed\n",
      "Value Function:\n",
      "[[ 6.86  4.32  2.72 ... 24.31 41.03 68.62]\n",
      " [ 4.32  2.72  1.72 ... 15.32 25.85 43.23]\n",
      " [ 2.72  1.72  1.08 ...  9.65 16.29 27.23]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy Iteration Converged after 9 iteration\n",
      "Reshaped Grid- Final Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 ... 1 1 0]\n",
      " [0 3 0 ... 1 1 0]\n",
      " [0 0 3 ... 1 1 0]\n",
      " ...\n",
      " [1 0 0 ... 0 0 1]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [2 0 1 ... 0 0 3]]\n",
      "\n",
      "Reshaped Grid- Final Value Function:\n",
      "[[ 6.86  4.32  2.72 ... 24.31 41.03 68.62]\n",
      " [ 4.32  2.72  1.72 ... 15.32 25.85 43.23]\n",
      " [ 2.72  1.72  1.08 ...  9.65 16.29 27.23]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python Modified_Policy_Iteration.py --grid_size=50 --prob_p=0.7 --stop_itr=10 --Final_Result=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "\n",
    "# Policy Iteration\n",
    "\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.    2.92  0.11 ...  1.28  2.3  99.96]\n",
      " [ 3.09  1.39  0.37 ...  2.22  7.71 29.68]\n",
      " [ 0.78  0.48  0.21 ...  1.42  3.87  9.22]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 1\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.    8.1   4.3  ... 43.02 53.12 99.96]\n",
      " [ 8.1   6.56  5.31 ... 53.12 65.58 80.97]\n",
      " [ 6.56  5.31  4.3  ... 43.02 53.12 65.58]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 2\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.    8.1   4.3  ... 43.02 53.12 99.96]\n",
      " [ 8.1   6.56  5.31 ... 53.12 65.58 80.97]\n",
      " [ 6.56  5.31  4.3  ... 43.02 53.12 65.58]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 3\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.    8.1   4.3  ... 43.02 53.12 99.96]\n",
      " [ 8.1   6.56  5.31 ... 53.12 65.58 80.97]\n",
      " [ 6.56  5.31  4.3  ... 43.02 53.12 65.58]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 4\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.    8.1   4.3  ... 43.02 53.12 99.96]\n",
      " [ 8.1   6.56  5.31 ... 53.12 65.58 80.97]\n",
      " [ 6.56  5.31  4.3  ... 43.02 53.12 65.58]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 5\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.    8.1   4.3  ... 43.02 53.12 99.96]\n",
      " [ 8.1   6.56  5.31 ... 53.12 65.58 80.97]\n",
      " [ 6.56  5.31  4.3  ... 43.02 53.12 65.58]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 6\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.    8.1   4.3  ... 43.02 53.12 99.96]\n",
      " [ 8.1   6.56  5.31 ... 53.12 65.58 80.97]\n",
      " [ 6.56  5.31  4.3  ... 43.02 53.12 65.58]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy Iteration Converged after 7 iteration\n",
      "Reshaped Grid- Final Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 2 ... 1 2 0]\n",
      " [0 0 3 ... 1 1 0]\n",
      " [0 0 0 ... 1 1 0]\n",
      " ...\n",
      " [1 0 1 ... 1 0 2]\n",
      " [1 0 1 ... 1 0 2]\n",
      " [1 0 1 ... 1 0 3]]\n",
      "\n",
      "Reshaped Grid- Final Value Function:\n",
      "[[10.    8.1   4.3  ... 43.02 53.12 99.96]\n",
      " [ 8.1   6.56  5.31 ... 53.12 65.58 80.97]\n",
      " [ 6.56  5.31  4.3  ... 43.02 53.12 65.58]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python Policy_Iteration.py --grid_size=50 --prob_p=0.9 --Final_Result=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[1.000e+01 1.690e+00 3.700e-01 ... 0.000e+00 0.000e+00 9.996e+01]\n",
      " [5.000e-02 3.100e-01 1.200e-01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.000e-02 6.000e-02 3.000e-02 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " ...\n",
      " [0.000e+00 0.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00 0.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 1\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.    6.3   3.97 ...  0.    0.   99.96]\n",
      " [ 2.5   3.97  2.5  ...  0.    0.    0.  ]\n",
      " [ 1.57  2.5   1.57 ...  0.    0.    0.  ]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 2\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.    6.3   3.97 ...  0.    0.   99.96]\n",
      " [ 2.5   3.97  2.5  ...  0.    0.    0.  ]\n",
      " [ 1.57  2.5   1.57 ...  0.    0.    0.  ]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 3\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.    6.3   3.97 ...  0.    0.   99.96]\n",
      " [ 2.5   3.97  2.5  ...  0.    0.    0.  ]\n",
      " [ 1.57  2.5   1.57 ...  0.    0.    0.  ]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 4\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.    6.3   3.97 ...  0.    0.   99.96]\n",
      " [ 2.5   3.97  2.5  ...  0.    0.    0.  ]\n",
      " [ 1.57  2.5   1.57 ...  0.    0.    0.  ]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 5\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.    6.3   3.97 ...  0.    0.   99.96]\n",
      " [ 2.5   3.97  2.5  ...  0.    0.    0.  ]\n",
      " [ 1.57  2.5   1.57 ...  0.    0.    0.  ]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 6\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.    6.3   3.97 ...  0.    0.   99.96]\n",
      " [ 2.5   3.97  2.5  ...  0.    0.    0.  ]\n",
      " [ 1.57  2.5   1.57 ...  0.    0.    0.  ]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 7\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.    6.3   3.97 ...  0.    0.   99.96]\n",
      " [ 2.5   3.97  2.5  ...  0.    0.    0.  ]\n",
      " [ 1.57  2.5   1.57 ...  0.    0.    0.  ]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 8\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.    6.3   3.97 ...  0.    0.   99.96]\n",
      " [ 2.5   3.97  2.5  ...  0.    0.    0.  ]\n",
      " [ 1.57  2.5   1.57 ...  0.    0.    0.  ]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy has not converged, updating new policy\n",
      "---------------------------------------------\n",
      "Epoch 9\n",
      "Running Policy Iteration; Value function Converged after 73 iterations\n",
      "Value Function:\n",
      "[[10.    6.3   3.97 ...  0.    0.   99.96]\n",
      " [ 2.5   3.97  2.5  ...  0.    0.    0.  ]\n",
      " [ 1.57  2.5   1.57 ...  0.    0.    0.  ]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Policy Iteration Converged after 10 iteration\n",
      "Reshaped Grid- Final Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 ... 2 3 0]\n",
      " [1 0 0 ... 3 3 3]\n",
      " [1 0 0 ... 2 2 3]\n",
      " ...\n",
      " [1 2 2 ... 2 0 2]\n",
      " [1 1 1 ... 3 3 3]\n",
      " [1 1 0 ... 0 0 3]]\n",
      "\n",
      "Reshaped Grid- Final Value Function:\n",
      "[[10.    6.3   3.97 ...  0.    0.   99.96]\n",
      " [ 2.5   3.97  2.5  ...  0.    0.    0.  ]\n",
      " [ 1.57  2.5   1.57 ...  0.    0.    0.  ]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python Policy_Iteration.py --grid_size=50 --prob_p=0.7 --Final_Result=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "#\n",
    "# Value Iteration\n",
    "#\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function after 1 iteration\n",
      "[[ 1.    0.81  0.66 ...  0.    0.   10.  ]\n",
      " [ 0.81  0.66  0.53 ...  0.    0.    8.1 ]\n",
      " [ 0.66  0.53  0.43 ...  0.    0.    6.56]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Value Function after 21 iteration\n",
      "[[ 8.91  7.21  5.84 ... 56.75 71.15 89.06]\n",
      " [ 7.21  5.84  4.73 ... 45.97 57.63 72.14]\n",
      " [ 5.84  4.73  3.83 ... 37.23 46.68 58.43]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Value Function after 41 iteration\n",
      "[[ 9.87  7.99  6.47 ... 64.53 79.8  98.67]\n",
      " [ 7.99  6.47  5.24 ... 52.27 64.64 79.92]\n",
      " [ 6.47  5.24  4.25 ... 42.34 52.36 64.74]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Value Function after 61 iteration\n",
      "[[ 9.98  8.09  6.55 ... 65.48 80.85 99.84]\n",
      " [ 8.09  6.55  5.31 ... 53.04 65.49 80.87]\n",
      " [ 6.55  5.31  4.3  ... 42.96 53.05 65.5 ]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Value Function after 81 iteration\n",
      "[[10.    8.1   6.56 ... 65.59 80.98 99.98]\n",
      " [ 8.1   6.56  5.31 ... 53.13 65.6  80.98]\n",
      " [ 6.56  5.31  4.3  ... 43.04 53.13 65.6 ]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Value Function after 101 iteration\n",
      "[[ 10.     8.1    6.56 ...  65.61  81.   100.  ]\n",
      " [  8.1    6.56   5.31 ...  53.14  65.61  81.  ]\n",
      " [  6.56   5.31   4.3  ...  43.05  53.14  65.61]\n",
      " ...\n",
      " [  0.     0.     0.   ...   0.     0.     0.  ]\n",
      " [  0.     0.     0.   ...   0.     0.     0.  ]\n",
      " [  0.     0.     0.   ...   0.     0.     0.  ]]\n",
      "\n",
      "Converged after 110 number of iterations\n",
      "Reshaped Grid- Final Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 ... 1 1 0]\n",
      " [0 0 0 ... 1 1 0]\n",
      " [0 0 0 ... 1 1 0]\n",
      " ...\n",
      " [0 0 1 ... 3 0 2]\n",
      " [0 0 1 ... 1 0 3]\n",
      " [0 0 1 ... 1 0 0]]\n",
      "\n",
      "Reshaped Grid- Final Value Function:\n",
      "[[ 10.     8.1    6.56 ...  65.61  81.   100.  ]\n",
      " [  8.1    6.56   5.31 ...  53.14  65.61  81.  ]\n",
      " [  6.56   5.31   4.3  ...  43.05  53.14  65.61]\n",
      " ...\n",
      " [  0.     0.     0.   ...   0.     0.     0.  ]\n",
      " [  0.     0.     0.   ...   0.     0.     0.  ]\n",
      " [  0.     0.     0.   ...   0.     0.     0.  ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python Value_Iteration.py --grid_size=50 --prob_p=0.9 --Final_Result=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function after 1 iteration\n",
      "[[ 1.    0.63  0.   ...  0.    0.   10.  ]\n",
      " [ 0.63  0.4   0.25 ...  0.    0.    0.  ]\n",
      " [ 0.4   0.25  0.16 ...  0.    0.    0.  ]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Value Function after 21 iteration\n",
      "[[ 8.91  5.61  1.38 ...  0.    0.   89.06]\n",
      " [ 5.61  3.53  2.23 ...  0.    0.    0.  ]\n",
      " [ 3.53  2.23  1.4  ...  0.    0.    0.  ]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Value Function after 41 iteration\n",
      "[[ 9.87  6.22  1.55 ...  0.    0.   98.67]\n",
      " [ 6.22  3.92  2.47 ...  0.    0.    0.  ]\n",
      " [ 3.92  2.47  1.55 ...  0.    0.    0.  ]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Value Function after 61 iteration\n",
      "[[ 9.98  6.29  1.57 ...  0.    0.   99.84]\n",
      " [ 6.29  3.96  2.5  ...  0.    0.    0.  ]\n",
      " [ 3.96  2.5   1.57 ...  0.    0.    0.  ]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Value Function after 81 iteration\n",
      "[[10.    6.3   1.57 ...  0.    0.   99.98]\n",
      " [ 6.3   3.97  2.5  ...  0.    0.    0.  ]\n",
      " [ 3.97  2.5   1.57 ...  0.    0.    0.  ]\n",
      " ...\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]\n",
      " [ 0.    0.    0.   ...  0.    0.    0.  ]]\n",
      "\n",
      "Value Function after 101 iteration\n",
      "[[ 10.     6.3    1.58 ...   0.     0.   100.  ]\n",
      " [  6.3    3.97   2.5  ...   0.     0.     0.  ]\n",
      " [  3.97   2.5    1.58 ...   0.     0.     0.  ]\n",
      " ...\n",
      " [  0.     0.     0.   ...   0.     0.     0.  ]\n",
      " [  0.     0.     0.   ...   0.     0.     0.  ]\n",
      " [  0.     0.     0.   ...   0.     0.     0.  ]]\n",
      "\n",
      "Converged after 110 number of iterations\n",
      "Reshaped Grid- Final Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 2 ... 1 2 0]\n",
      " [0 3 3 ... 3 3 3]\n",
      " [0 0 0 ... 0 2 2]\n",
      " ...\n",
      " [2 0 0 ... 0 3 2]\n",
      " [1 1 1 ... 3 0 3]\n",
      " [0 1 1 ... 0 3 3]]\n",
      "\n",
      "Reshaped Grid- Final Value Function:\n",
      "[[ 10.     6.3    1.58 ...   0.     0.   100.  ]\n",
      " [  6.3    3.97   2.5  ...   0.     0.     0.  ]\n",
      " [  3.97   2.5    1.58 ...   0.     0.     0.  ]\n",
      " ...\n",
      " [  0.     0.     0.   ...   0.     0.     0.  ]\n",
      " [  0.     0.     0.   ...   0.     0.     0.  ]\n",
      " [  0.     0.     0.   ...   0.     0.     0.  ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python Value_Iteration.py --grid_size=50 --prob_p=0.7 --Final_Result=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################\n",
    "\n",
    "# Report:\n",
    "\n",
    "#######################\n",
    "\n",
    "### When we're considering the 50X50, value function of upper right terminal states and states abound it seems to higher value and it persist for all experiments, However as our grid world is now large whenever we end up around the upper left corner even though value function of the upper left corner is low compare to the upper right terminal, agent will terminate at upper left. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
